{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suchismita-priya/bike_sharing_demand_prediction/blob/main/Bike_sharing_demand_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Suchismita Priyadarsinee"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues.\n",
        "\n",
        "Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.\n",
        "\n",
        "\n",
        "**Data Description**\n",
        "The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.\n",
        "\n",
        "Attribute Information: Date : year-month-day\n",
        "\n",
        "Rented Bike count - Count of bikes rented at each hour\n",
        "\n",
        "Hour - Hour of he day\n",
        "\n",
        "Temperature-Temperature in Celsius\n",
        "\n",
        "Humidity - %\n",
        "\n",
        "Windspeed - m/s\n",
        "\n",
        "Visibility - 10m\n",
        "\n",
        "Dew point temperature - Celsius\n",
        "\n",
        "Solar radiation - MJ/m2\n",
        "\n",
        "Rainfall - mm\n",
        "\n",
        "Snowfall - cm\n",
        "\n",
        "Seasons - Winter, Spring, Summer, Autumn\n",
        "\n",
        "Holiday - Holiday/No holiday\n",
        "\n",
        "Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)"
      ],
      "metadata": {
        "id": "0QWTYOS-tQHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from scipy import stats\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score ,accuracy_score,roc_auc_score,roc_curve\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "\n",
        "!pip install shap\n",
        "import shap"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "AJ84WAcjMiw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "file_path= '/content/drive/MyDrive/SeoulBikeData.csv'\n",
        "\n",
        "# Reading CSV file.\n",
        "original_df=pd.read_csv(file_path,encoding=\"latin\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "original_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = original_df.copy()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "lyq8Q5tFE00S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "rows_col_count = df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of rows are', rows_col_count[0] )\n",
        "print('Number of columns are', rows_col_count[1] )"
      ],
      "metadata": {
        "id": "YRPIKJ7gWL91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.   Here we can see we have 8760 rows and 14 columns.\n",
        "\n",
        "2.   we don't have any null value.\n",
        "\n",
        "3.   Columns to convert:\n",
        "      Date - proper datetime format\n",
        "      Seasons,Holiday,Functioning Day - these all need to convert to numeric."
      ],
      "metadata": {
        "id": "lqjOLiLDY207"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "jTBmg1oGays5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As it would be difficult from all these column names to copy , so we can rename the columns\n",
        "df.rename(columns={'Rented Bike Count':'Rented Bike Count','Temperature(°C)':'Temperature','Humidity(%)':'Humidity','Wind speed (m/s)':'Wind_speed',\n",
        "                       'Visibility (10m)':'Visibility','Dew point temperature(°C)':'Dew_point_temperature', 'Solar Radiation (MJ/m2)':'Solar_Radiation',\n",
        "                        'Rainfall(mm)':'Rainfall','Snowfall (cm)':'Snowfall','Functioning Day':'Functioning_Day'},inplace=True)\n",
        "df.columns"
      ],
      "metadata": {
        "id": "mjbtVEBRbCm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When there are identical instances in a dataset, it is referred to as \"duplication.\" Such duplication could occur due to erroneous data entry or data collection procedures. Eliminating duplicate data from the dataset saves time and money by avoiding the repetition of the same data sent to the machine learning model."
      ],
      "metadata": {
        "id": "mx6Zgph0doHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found that there is no duplicate entry in the above data."
      ],
      "metadata": {
        "id": "Jb-q4z-gdhN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why dealing with missing values is necessary?\n",
        "\n",
        "Real-world data often contains numerous missing values, which can be due to data corruption or other factors. As many machine-learning algorithms do not support missing values, it is necessary to handle them during the dataset pre-processing stage. Thus, the first step in dealing with missing data is to identify the missing values."
      ],
      "metadata": {
        "id": "6df46RFhdb1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import missingno as msno\n",
        "msno.bar(df)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From our dataset it is clear that there is no null value present and also no duplicated values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.\n",
        "\n",
        "**Attribute Information:**\n",
        "\n",
        "Date :year-month-day\n",
        "\n",
        "Rented Bike count :Count of bikes rented at each hour\n",
        "\n",
        "Hour :Hour of the day\n",
        "\n",
        "Temperature :Temperature in Celsius\n",
        "\n",
        "Humidity :%\n",
        "\n",
        "Windspeed :m/s\n",
        "\n",
        "Visibility :10m\n",
        "\n",
        "Dew point temperature :Celsius\n",
        "\n",
        "Solar radiation :MJ/m2\n",
        "\n",
        "Rainfall :mm\n",
        "\n",
        "Snowfall :cm\n",
        "\n",
        "Seasons :Winter, Spring, Summer, Autumn\n",
        "\n",
        "Holiday :Holiday/No holiday\n",
        "\n",
        "Functional Day :NoFunc(Non Functional Hours), Fun(Functional hours)"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are focusing on several key columns of our dataset, including 'Hour', 'Holiday', 'Functioning Day', 'Rented Bike Count', 'Temperature(°C)', and 'Seasons', as they contain a wealth of information.\n",
        "\n",
        "By utilizing these features, we plan to create a regression model and implement various regression algorithms.\n",
        "\n",
        "There is a column 'Hour' which might be considered a categorical feature or maybe a numerical feature based on the data we will try both and see the result difference."
      ],
      "metadata": {
        "id": "hdif9DOHsR6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data wrangling, also known as data preprocessing, refers to the process of cleaning, structuring, and transforming raw data into a format suitable for analysis or other data-related tasks. It is a crucial step in the data preparation pipeline, as raw data often comes in various forms and may have inconsistencies, missing values, and other issues that need to be addressed before meaningful analysis can take place.\n",
        "\n",
        "Key tasks involved in data wrangling include:\n",
        "\n",
        "**Data Cleaning**: Identifying and handling missing data, outliers, duplicates, and errors. This may involve imputing missing values, removing duplicate records, and correcting inaccuracies.\n",
        "\n",
        "**Data Transformation**: Converting data into a common format, such as changing data types, normalizing or scaling numeric values, and encoding categorical variables into numerical values (e.g., one-hot encoding).\n",
        "\n",
        "**Data Integration**: Combining data from multiple sources or files into a unified dataset. This may involve merging datasets, aligning data based on common keys, or aggregating data at different levels.\n",
        "\n",
        "**Data Reduction**: Reducing the dimensionality of data by selecting relevant features or variables and discarding irrelevant or redundant ones. This can improve the efficiency of modeling and analysis.\n",
        "\n",
        "**Data Exploration**: Exploring and visualizing the data to gain insights, identify patterns, and detect relationships. This helps in making informed decisions during the wrangling process.\n",
        "\n",
        "Data wrangling is a critical step in the data analysis pipeline because the quality and structure of the data significantly impact the accuracy and reliability of the insights and models derived from it. Effective data wrangling ensures that the data is ready for analysis, reporting, or machine learning tasks."
      ],
      "metadata": {
        "id": "5-2WN84etbrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Breaking down date column\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "\n",
        "df[\"weekday\"] = df[\"Date\"].dt.day_name()\n",
        "df[\"Day\"] = df[\"Date\"].dt.day\n",
        "df[\"Month\"] = df[\"Date\"].dt.month\n",
        "df[\"Year\"] = df[\"Date\"].dt.year\n",
        "\n",
        "df.drop(\"Date\", axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "WL5_meO8tOhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the Newly Created columns\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "unQU9pXevTNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "_yEpoIlCB-hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the data we knew that there is no null or duplicate value.so we don't need to treat that.\n",
        "Date should be in datatime format.\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_vars = list(df.select_dtypes(exclude=['object']))\n",
        "num_vars"
      ],
      "metadata": {
        "id": "nnsGCKuNMPHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for i in num_vars:\n",
        "#  sns.boxplot(y = df[i])\n",
        "#  plt.show()"
      ],
      "metadata": {
        "id": "C0pd-hClQkVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# priting box plot of numeric columns to check oultliers\n",
        "for i in num_vars:\n",
        "  fig = plt.figure(figsize=(5,3))\n",
        "  ax = fig.gca()\n",
        "  sns.boxplot(y = df[i] , ax=ax)\n",
        "  ax.set_title(i)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Numeric variable some plots are good from this boxplot is there.\n",
        "This comes under univariate analysis.On this we will see indivisual variables one at a time."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers are visible in the 'wind_speed', and 'solar_radiation' columns.\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes from above we got to know on which dataset there are outliers and in below steps we'll remove these outliers to make data more efficient and model ready."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Histogram\n",
        "for i in num_vars:\n",
        "  sns.histplot(x = df[i])\n",
        "  plt.axvline(df[i].mean(), color='g', linestyle='dashed', linewidth=2)\n",
        "  plt.axvline(df[i].median(), color='red', linestyle='dashed', linewidth=2)\n",
        "  plt.title(i)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "They allow you to see how data is spread out, whether it's symmetric or skewed, and whether there are any outliers.\n",
        "Outliers often appear as data points that are far from the bulk of the distribution, making them visually conspicuous.\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the distributions are rightly skewed while some are leftly skewed.\n",
        "The features which are skewed, their mean and the median are also skewed."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes some are left,right skewed so in coming section we'll treat skeweness."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# normalise above chart"
      ],
      "metadata": {
        "id": "KKBgZfSNj_bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing The variables using squre root method.\n",
        "for i in num_vars:\n",
        "    feature = np.sqrt(df[i])\n",
        "    feature.hist(bins=50)\n",
        "    plt.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "    plt.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "    plt.title(i)\n",
        "    plt.xlabel(i)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ggKvuWEDkRt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(2)"
      ],
      "metadata": {
        "id": "2FPUyDfAu51n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Both chart 1 & chart 2 are for numeric variable analysis.(univariate analysis)\n",
        "#   1-Histogram\n",
        "#   2-Boxplot\n",
        "# chart 3 & chart 4 will be for catagorical variable(univriate analysis)\n",
        "#   1-Barchart\n",
        "#   2-Piechart"
      ],
      "metadata": {
        "id": "1dANBp_JmLfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Bar chart\n",
        "cat_cols = list(df.select_dtypes(include=['object']))\n",
        "cat_cols"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in cat_cols:\n",
        "  plt.figure(figsize=(5,3))\n",
        "  df[i].value_counts().plot(kind='bar')\n",
        "  plt.xlabel(i)\n",
        "  plt.ylabel('count')\n",
        "  plt.title(i)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "O5TWG06yoiQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts make it easy to compare different categories or groups of data. You can quickly see which category has the highest or lowest value.\n",
        "bar charts are a versatile and widely used tool for data visualization, particularly when dealing with categorical or discrete data. They provide a clear visual representation of data, making it easier to draw insights and make informed decisions."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No holiday,function day, summer is having maximum counts."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "as these above variables are high , we'll see how these will react with rented_bike_count on bivariate analysis."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# piechart\n",
        "for i in cat_cols:\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    df[i].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
        "    plt.title(f'Distribution of {i}')\n",
        "    plt.ylabel('')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pie charts are a type of data visualization used to represent categorical data as a circular graph. They are useful for visualizing the distribution of categories or proportions of a whole.\n",
        "Each slice of the pie represents a category, and the whole pie represents 100% of the data.\n",
        "Pie charts are simple to understand and visually appealing. They work well when you have a small number of categories."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution of catagorical variables showing above in chart."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the annotation we can clearly see which are the variables which are high in count and we can consider these in further for analysis."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Bivariate analysis(numeric vs numeric)\n",
        "# Regression plot\n",
        "for i in num_vars:\n",
        "  #plt.scatter(x= df[i], y=df['Rented Bike Count'])\n",
        "  sns.regplot(x = df[i], y = df['Rented Bike Count'],line_kws = {\"color\": \"red\"})\n",
        "  plt.title(i)\n",
        "  plt.xlabel(i)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a regression plot, you typically have a scatter plot of the data points, similar to a regular scatter plot. However, regression plots also include a fitted regression line (a straight line or curve) that represents the best-fit relationship between the variables. This line is determined by a regression algorithm."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This regression plots shows that some of our features are positive linear and some are negative linear in relation to our target variable.\n",
        "\n",
        "\n",
        "**Hour:**\n",
        "1)There is sudden peak between 6/7AM to 10 AM. Office time,College and going time could be the reason for this sudden peak.\n",
        "\n",
        "2) Again there is peak between 10 AM to 7 PM. may be its office leaving time for the above people.\n",
        "\n",
        "3) We can say that,from morning 7 AM to Evening 7 PM we have good Bike Rent Count. and from 7 PM to 7 AM Bike Rent count starts declining.\n",
        "\n",
        "**Temperature:**\n",
        "1) For decrease in temperature below 0 deg celicus the bike rent count is significantly decreased because may be people dont want to ride bike in such cold temperature.\n",
        "\n",
        "2) But for normal temperature the Bike rent count is very high.\n",
        "\n",
        "**humidity**\n",
        "1) Here its seems like humidty is inversely proportional to bike rent count. As humdity percentage is increasing there is decrease in bike rent count.\n",
        "\n",
        "**Wind Speed:**\n",
        "1) upto wind speed 4 m/s there is good bike rent count.\n",
        "\n",
        "**Visibility**\n",
        "1) It's very obivious that as visibilty increases the bike rent count also increases. Nobody would prefere to ride in low visibilty.\n",
        "\n",
        "**Dew Point Temperature**\n",
        "1)It's again the same case as of temperature. As dew temperature goes below 0 deg celcius there is less bike rent count. It looks like Dew Point temperature and Temperature columns have strong colinarity.\n",
        "\n",
        "**Solar radiation**\n",
        "1)Here the amount of rented bikes is huge, when there is solar radiation.\n",
        "\n",
        "**Rainfall And snowfall**\n",
        "1) Its very obivious that people usually do not like ride bikes in rain and snowfall."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can check linearity with dependent and independent variables."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# scatterplot and regressionplot are kind of same but with regression plot bestfitlines are showing."
      ],
      "metadata": {
        "id": "-zgz0HUYMnEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# jointplot/pairplot\n",
        "\n",
        "sns.pairplot(df)\n",
        "plt.figure(figsize=(30,25))"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pairplot is a type of data visualization that is particularly useful when dealing with datasets that contain multiple numerical features (variables). It provides a grid of scatterplots for each pair of numerical variables in your dataset, allowing you to visualize the relationships and correlations between them. Here are some reasons why you might want to use a pairplot:\n",
        "\n",
        "**Identify Patterns:** Pairplots help you quickly identify any visible patterns or trends in your data. You can visually detect relationships such as linear, nonlinear, or clustering patterns between pairs of variables.\n",
        "\n",
        "**Correlation Analysis:** You can use pairplots to assess the degree and direction of correlation between variables. If two variables show a linear relationship, you'll see a trend or a line in the scatterplot.\n",
        "\n",
        "**Outlier Detection:** Outliers, which are data points that significantly deviate from the majority of the data, can be spotted easily in pairplots. These outliers may warrant further investigation or data preprocessing.\n",
        "\n",
        "**Feature Selection:** When you have many variables, pairplots can help you decide which variables might be worth exploring further in your analysis. You can look for relationships between features and the target variable.\n",
        "\n",
        "**Data Exploration:** Pairplots are a useful exploratory data analysis (EDA) tool. They allow you to get a sense of the data distribution and relationships before building more complex models.\n",
        "\n",
        "**Multivariate Analysis:** While scatterplots show relationships between two variables at a time, pairplots extend this analysis to multiple variables simultaneously. This can be particularly valuable when dealing with high-dimensional data.\n",
        "\n",
        "**Diagnostic Tool:** Pairplots can be used as a diagnostic tool to identify potential issues such as heteroscedasticity, nonlinearity, or interaction effects between variables.\n",
        "\n",
        "**Communication:** When presenting your findings or insights from a dataset, pairplots can be a helpful visualization to convey complex relationships in a simple and interpretable manner."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to find on which time our demand for bike rent is increasing. from the chart we get that,\n",
        "- Snowfall & Rainfall are highly corelated to each other, so in future at time of model training we'll use only one .because if we'll use both then we'll give more data to train the model it will take extra times.\n",
        "As both are giving same information so we can use one from both.\n",
        "- Rented bike count are right skewed."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " -  When we'll see hour column with our dependent variable , there are some hours on which the demand of our bikes are high. so on that company can increase the price, so it can give good impact in business.\n",
        " -  also at the time when demand are low in month or day or hour company can give their bike for service."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Box plot (Bivariate, numeric vs categorical)\n",
        "for i in cat_cols:\n",
        "  sns.boxplot(x=df[i],y = df['Rented Bike Count'])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A boxplot displays the median (the middle value), quartiles (25th and 75th percentiles), and potential outliers. This summary provides a quick understanding of the data's central tendency and spread.\n",
        "\n",
        "Outliers are data points that significantly deviate from the bulk of the data. Boxplots explicitly show potential outliers as individual points outside the \"whiskers\" or the interquartile range (IQR). Outliers can be critical in data analysis, as they may indicate errors or interesting phenomena.\n",
        "\n",
        " A long tail on one side of the box indicates skewness in that direction.\n",
        "\n",
        " The length of the box (the IQR) represents the spread or variability of the data. A longer box indicates greater variability."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Summer season had the higest Bike Rent Count. People are more likely to rent bikes in summer. Bike rentals in winter is very less compared to other seasons.\n",
        "\n",
        "2) High number of bikes were rented on No Holidays.\n",
        "\n",
        "3) On no functioning day no bikes were rented. we have only 295 counts of \"NO\". Thus we can drop that 295 values but column having only \"Yes' will not add value to our column.\n"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "no function column will not give any value,So these columns is not use full for us. We will drop those in next steps.\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Bar chart\n",
        "for i in cat_cols:\n",
        "    df.groupby(i)['Rented Bike Count'].size().plot(kind='bar')\n",
        "    plt.xlabel(i)\n",
        "    plt.ylabel('Rented Bike Count')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plots are excellent for comparing different categories or groups. They make it easy to see and understand the differences in values among these categories.\n",
        " Bar plots are easy to read and interpret, even for individuals who are not familiar with data visualization techniques. They provide a clear and straightforward representation of data."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in function day rented bike count are high.\n",
        "\n",
        "in summer & spring season count are high."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above chart company can give more importance on summer ,functions day."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Bike Rental trend with respect to Hour on Holiday or No Holiday.\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.pointplot(x=df['Hour'],y=df['Rented Bike Count'],hue=df['Holiday'])\n",
        "plt.title(\"Bike Rental Trend according to Hour on Holiday / No Holiday\")\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Point plots are used to display trends, patterns, or relationships in data. They are particularly effective when you want to emphasize variations in data across different categories or levels of an independent variable.\n",
        "\n",
        "They provide a clear visual representation of how the central tendency (e.g., mean, median) of the data varies across categories."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) There is sudden peak between 6/7AM to 10 AM. Office time,College and going time could be the reason for this sudden peak.( NO Holiday). But on Holiday the case is different less bike rentals happend.\n",
        "\n",
        "2) Again there is peak between 10 AM to 7 PM. may be its office leaving time for the above people.( NO Holiday)."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- office,school timing are for good business time."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "## Bike Rental trend with respect to Hour on Functioning day.\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.pointplot(x = df['Hour'], y=df['Rented Bike Count'], hue=df['Functioning_Day'])\n",
        "plt.title('Distribution of rental bike count on functioning day wrt hour')"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Point plots are used to display trends, patterns, or relationships in data. They are particularly effective when you want to emphasize variations in data across different categories or levels of an independent variable.\n",
        "\n",
        "They provide a clear visual representation of how the central tendency (e.g., mean, median) of the data varies across categories."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Here the trend for functioning day is same as of No holiday. Only the difference is on No functioning day there is no bike rentals.\n",
        "\n",
        "*(if we drop no fucntioning day rows. we will left only with the functioning day values.thus the column having same value through out will not be helpful.*\n",
        "*So,its better to drop the column.)*"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will not create any business need as we will train that with model."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# Bike Rent Count trend with respect Hours on Seasons.\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.pointplot(x=df['Hour'],y=df['Rented Bike Count'],hue=df['Seasons'])\n",
        "plt.title('bike rent count on seasons wrt hour')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "point plots are valuable for highlighting patterns, differences, and relationships in data, especially when comparing means or averages across categorical variables. They are a versatile tool in the data analyst's toolbox for exploring and communicating data insights effectively.\n",
        "\n",
        "Point plots often include error bars that represent confidence intervals or standard errors around the central values. These error bars provide information about the uncertainty associated with each point estimate, enhancing the viewer's understanding of the data."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On winter bike rents are high."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "on summer more bike should make available."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Let's check distribution of target variable- \"Bike Rented Count\"\n",
        "f, axes = plt.subplots(1,2,figsize=(10,6))\n",
        "sns.distplot(x=df['Rented Bike Count'],color='r',ax=axes[0])\n",
        "sns.boxplot(x=df['Rented Bike Count'],color='r',ax=axes[1])"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These two we have used just to check the distribution of dependent variable and it's skewness."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got to know from the two chart these are rightly skewed."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we will see that these are skewed ,to normalize them we'll use square root method."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Normalizing  our target variable by squre root method\n",
        "f,axes = plt.subplots(1,2,figsize=(10,4))\n",
        "sns.distplot(x=np.sqrt(df['Rented Bike Count']),color='g',ax=axes[0])\n",
        "sns.boxplot(x=np.sqrt(df['Rented Bike Count']),color='g',ax=axes[1])"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have done distplot chart & boxplot chart to check for skewness."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "on above we have done square root normalization ,after that there is changes in figure as before it was right skewed but now it is not showing skewed & no outliers present.\n"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "having outlier would have created problem in model training , after normalization outlier were not present hence it will give good impact on training."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Checking Corelation between dependent and independent variable\n",
        "plt.figure(figsize=(20,8))\n",
        "sns.heatmap(df.corr(),annot=True,cmap='coolwarm')\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is particularly useful for visualizing complex datasets, especially when you want to explore relationships between variables or patterns within the data. Heatmaps are excellent for visualizing relationships or correlations between variables in a dataset. By assigning colors to different values, you can quickly identify which variables are positively or negatively correlated.\n",
        "Unusual or outlier values can stand out in a heatmap"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Temperature and Dew point Temperature are highly correlated.\n",
        "As per our regression assumption, there should not be colinearity between independent variables.\n",
        "\n",
        "- We can see from the heatmap that \"Temperature\" and \"Dew Point Temperature\" are highly corelated. We can drop one of them.As the corelation between temperature and our dependent variable \"Bike Rented Count\" is high. So we will Keep the Temperature column and drop the \"Dew Point Temperature\" column.\n",
        "\n",
        "- And also we dropping \"Date\" column as its not useful for us(as we have created the month, day and year columns."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df)"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pairplot is a type of data visualization that is particularly useful when dealing with datasets that contain multiple numerical features (variables). It provides a grid of scatterplots for each pair of numerical variables in your dataset, allowing you to visualize the relationships and correlations between them."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to find on which time our demand for bike rent is increasing. from the chart we get that,\n",
        "\n",
        "Snowfall & Rainfall are highly corelated to each other, so in future at time of model training we'll use only one .because if we'll use both then we'll give more data to train the model it will take extra times. As both are giving same information so we can use one from both.\n",
        "Rented bike count are right skewed."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis Statement 1:**\n",
        "Statement: The average number of bike rentals on function day is higher than non-function day.\n",
        "\n",
        "Null Hypothesis (H0): μ_function ≤ μ_nonfunction\n",
        "\n",
        "Alternative Hypothesis (H1): μ_function > μ_nonfunction\n",
        "\n",
        "**Hypothesis Statement 2:**\n",
        "Statement: There is no significant difference in the average number of bike rentals during different weather conditions (clear vs. not clear).\n",
        "\n",
        "Null Hypothesis (H0): μ_clear = μ_not_clear\n",
        "\n",
        "Alternative Hypothesis (H1): μ_clear ≠ μ_not_clear\n",
        "\n",
        "**Hypothesis Statement 3:**\n",
        "Statement: Bike rentals on holidays and non-holidays have the same average demand.\n",
        "\n",
        "Null Hypothesis (H0): μ_holiday = μ_non_holiday\n",
        "\n",
        "Alternative Hypothesis (H1): μ_holiday ≠ μ_non_holiday"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statement:** The average number of bike rentals on function day is higher than non-function day.\n",
        "\n",
        "**Null Hypothesis (H0):**\n",
        "The null hypothesis assumes that there is no significant difference in the average number of bike rentals between function and nonfunction. It states that the average number of rentals on function (μ_function) is less than or equal to the average number of rentals on nonfunction (μ_nonfunction).\n",
        "\n",
        "**Alternative Hypothesis (H1):**\n",
        "The alternative hypothesis suggests that there is a significant difference in the average number of bike rentals between function and nonfunction. It specifically states that the average number of rentals on function (μ_function) is greater than the average number of rentals on nonfunction (μ_nonfunction).\n",
        "\n",
        "**In statistical terms:**\n",
        "\n",
        "H0: μ_function ≤ μ_nonfunction\n",
        "H1: μ_function > μ_nonfunction\n",
        "The goal of the hypothesis test is to determine whether there is enough evidence to reject the null hypothesis in favor of the alternative hypothesis based on the sample data."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(2)"
      ],
      "metadata": {
        "id": "bwEotUX0A4wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothesis Test 1 - T-test\n",
        "Functioning_Day_rentals = df[df['Functioning_Day'] == 'Yes']['Rented Bike Count']\n",
        "Non_Functioning_Day_rentals = df[df['Functioning_Day'] == 'No']['Rented Bike Count']\n",
        "t_statistic1, p_value1 = stats.ttest_ind(Functioning_Day_rentals, Non_Functioning_Day_rentals, alternative='greater')"
      ],
      "metadata": {
        "id": "3vkpq3CRxUuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hypothesis Test 1 (Functioning_Day vs. Non_Functioning_Day): p-value (T-test) =\", p_value1)"
      ],
      "metadata": {
        "id": "4ZO9-2nJEdIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p-value of 3.366e-83, or 3.366105287750711e-83 in scientific notation, is an extremely small value. In hypothesis testing, the p-value is used to determine the significance of your results.\n",
        "\n",
        "Here's what this p-value means in the context of Hypothesis Test 1 (Functioning_Day vs. Non_Functioning_Day):\n",
        "\n",
        "**Null Hypothesis (H0):** There is no significant difference in bike demand between Functioning Days and Non-Functioning Days.\n",
        "\n",
        "**Alternative Hypothesis (Ha):** There is a significant difference in bike demand between Functioning Days and Non-Functioning Days.\n",
        "\n",
        "When you perform a t-test and obtain a p-value as small as 3.366e-83, it indicates an extremely high level of statistical significance. This means that the data provides very strong evidence against the null hypothesis. In other words, you can conclude that there is indeed a significant difference in bike demand between Functioning Days and Non-Functioning Days.\n",
        "\n",
        "In practical terms, this result suggests that the day of the week (Functioning vs. Non-Functioning) has a statistically significant impact on bike demand, and you should reject the null hypothesis in favor of the alternative hypothesis."
      ],
      "metadata": {
        "id": "wlmpVNqqH1KA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test that was performed to obtain the p-value in Hypothesis Test 1 (Functioning_Day vs. Non_Functioning_Day) is a t-test."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific statistical test, a t-test, was chosen because it is appropriate for comparing the means of two groups to determine if they are significantly different from each other.\n",
        "\n",
        "In Hypothesis Test 1, the goal was to compare the average bike demand on functioning days (Functioning_Day == Yes) and non-functioning days (Functioning_Day == No) to see if there was a statistically significant difference in bike demand between these two groups.\n",
        "\n",
        "The t-test is a commonly used test for such comparisons, and it helps determine if the observed differences in means are statistically significant or if they could have occurred due to random chance.\n",
        "\n",
        "The resulting low p-value (p < 0.05) suggests that there is strong evidence to reject the null hypothesis and conclude that there is a significant difference in bike demand between functioning and non-functioning days."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 2 (Temperature Impact on Bike Demand):\n",
        "\n",
        "**Null Hypothesis (H0):** The average bike demand on days with high temperatures (above a certain threshold) is equal to the average bike demand on days with low temperatures (below or equal to the threshold).\n",
        "\n",
        "**Alternate Hypothesis (Ha):** The average bike demand on days with high temperatures (above a certain threshold) is not equal to the average bike demand on days with low temperatures (below or equal to the threshold).\n",
        "\n",
        "In other words, the null hypothesis assumes that temperature has no significant impact on bike demand, while the alternate hypothesis suggests that there is a significant difference in bike demand between days with high and low temperatures."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(2)"
      ],
      "metadata": {
        "id": "9F0M80-9Kj93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Hypothesis Test 2 - Chi-square Test\n",
        "weather_cross_tab = pd.crosstab(df['Seasons'], df['Rented Bike Count'])\n",
        "chi2_stat2, p_value2, _, _ = stats.chi2_contingency(weather_cross_tab)"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hypothesis Test 2 (Clear vs. Not Clear Weather): p-value (Chi-square test) =\", p_value2)"
      ],
      "metadata": {
        "id": "2WDTEn-IKTdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Chi-square test to obtain the p-value in Hypothesis Test 2 (Clear vs. Not Clear Weather)."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Chi-square test was chosen for Hypothesis Test 2 (Clear vs. Not Clear Weather) because it's appropriate for analyzing the association between two categorical variables, such as \"Clear\" or \"Not Clear\" weather conditions. In this test, we are comparing the observed frequencies of these categorical variables against what would be expected if there were no association (null hypothesis).\n",
        "\n",
        "The Chi-square test helps determine whether the observed distribution of data significantly differs from the expected distribution, and the resulting p-value indicates the strength of evidence against the null hypothesis. In this case, a very low p-value (1.703789312477651e-127) suggests a strong association between weather conditions and bike demand, indicating that \"Clear\" and \"Not Clear\" weather have significantly different effects on bike demand."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bike rentals on holidays and non-holidays have the same average demand.\n",
        "\n",
        "**Null Hypothesis (H0):** Bike rentals on holidays and non-holidays have the same average demand.\n",
        "\n",
        "**Alternate Hypothesis (H1):** Bike rentals on holidays and non-holidays have different average demands."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Hypothesis Test 3 - ANOVA Test\n",
        "# Split the data into two groups: Holiday and Non-Holiday\n",
        "holiday_group = df[df['Holiday'] == 'Holiday']['Rented Bike Count']\n",
        "non_holiday_group = df[df['Holiday'] == 'No Holiday']['Rented Bike Count']\n",
        "\n",
        "# Perform ANOVA test\n",
        "f_statistic, p_value3 = stats.f_oneway(holiday_group, non_holiday_group)\n",
        "\n",
        "# Print the p-value\n",
        "print(f'p-value (ANOVA) for Holiday vs. Non-Holiday: {p_value3}')"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used ANOVA Test."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific statistical test chosen for Hypothesis Test 3 (Bike rentals on holidays and non-holidays have the same average demand) is the Analysis of Variance (ANOVA) test. ANOVA is an appropriate choice for this hypothesis for the following reasons:\n",
        "\n",
        "Comparison of Multiple Groups: ANOVA allows us to compare the means of more than two groups, making it suitable for comparing demand on multiple days of the week, such as holidays and non-holidays.\n",
        "\n",
        "Numeric Data: ANOVA is used to compare means of numeric data across different groups. In this case, we are comparing the average demand (numeric data) across the two groups (holidays and non-holidays).\n",
        "\n",
        "Assumption of Independence: ANOVA assumes that observations within each group are independent. In the context of bike rentals, it's reasonable to assume that the demand on one day is independent of the demand on another day.\n",
        "\n",
        "Assumption of Homogeneity of Variances: ANOVA assumes that the variances within each group are approximately equal. This assumption is often met in practice when analyzing real-world data.\n",
        "\n",
        "Hypothesis Testing: ANOVA allows us to test whether there are statistically significant differences in means across groups. In this case, we want to test if the mean demand on holidays is significantly different from the mean demand on non-holidays.\n",
        "\n",
        "Overall, ANOVA is a suitable test for comparing means across multiple groups, making it a logical choice for examining whether there is a significant difference in average bike demand between holidays and non-holidays in the dataset."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our dataset as there is no null value so we don't need to do missing value imputation."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for outliers\n",
        "for i in num_vars:\n",
        "  sns.boxplot(df[i])\n",
        "  plt.xlabel(i)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Gy0K_-_0hFHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers are visible in the 'wind_speed', and 'solar_radiation' columns."
      ],
      "metadata": {
        "id": "OEM2bZh4jhXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.copy()"
      ],
      "metadata": {
        "id": "Ii24zE01FLtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head(2)"
      ],
      "metadata": {
        "id": "ZKWTsErjFO7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# clipping method\n",
        "def clip_outliers(df2):\n",
        "  for i in df2[num_vars]:\n",
        "    # using IQR method to define range of upper and lower limit.\n",
        "    q1 = df2[i].quantile(0.25)\n",
        "    q3 = df2[i].quantile(0.75)\n",
        "    iqr = q3-q1\n",
        "    lowerbound = q1-1.5*iqr\n",
        "    upperbound = q3+1.5*iqr\n",
        "    # replacing the outliers with upper and lower bound\n",
        "    df2[i] = df2[i].clip(lowerbound,upperbound)\n",
        "  return df2"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = clip_outliers(df2)"
      ],
      "metadata": {
        "id": "5nRulhf9lya4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in num_vars:\n",
        "  sns.boxplot(x=df2[i])\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "BpJHIf_cmGmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have limited datapoint hence we are not simply removing the outlier instead of that we are using the clipping method."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(2)"
      ],
      "metadata": {
        "id": "ghWRkchXJR18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "category_columns = list(df.select_dtypes(include=object))\n",
        "category_columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in category_columns:\n",
        "  print('no of unique values in',i,'is',df[i].nunique())"
      ],
      "metadata": {
        "id": "ANY9NXB0Lkzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use one hot encoding for 'seasons','weekday' and Numeric encoding for 'holiday' and 'functioning_day'. Other columns are already encoded."
      ],
      "metadata": {
        "id": "KXCzvhHAN8Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical encoding\n",
        "df[\"Holiday\"] = df[\"Holiday\"].map({\"No Holiday\":0, \"Holiday\":1})\n",
        "df[\"Functioning_Day\"] = df[\"Functioning_Day\"].map({\"No\":0, \"Yes\":1})"
      ],
      "metadata": {
        "id": "Ii05RWEbmG4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encoding\n",
        "df_season = pd.get_dummies(df[\"Seasons\"], drop_first = True)\n",
        "df_weekday = pd.get_dummies(df[\"weekday\"], drop_first = True)"
      ],
      "metadata": {
        "id": "sdyMHtwsNv6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate the one-hot encoded season,weekday feature with the rest of the data\n",
        "#df = pd.concat([df, df_season], axis=1)\n",
        "#df = pd.concat([df, df_weekday], axis=1)\n",
        "df = pd.concat([df, df_season, df_weekday], axis=1)"
      ],
      "metadata": {
        "id": "tRv4ypbDEQFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the original features\n",
        "#df.drop('Seasons', axis=1, inplace=True)\n",
        "#df.drop('weekday', axis=1, inplace=True)\n",
        "df.drop([\"Seasons\", \"weekday\"], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "GZ5DTA1PEUM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "g5k1uF0oPePi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "XOE-_g55eYZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the option to display all columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "m36crObATGJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding is a technique in feature engineering that is used to convert categorical variables into numerical values that can be used by machine learning algorithms.\n",
        "\n",
        "There are several encoding techniques, including:\n",
        "\n",
        "**One-hot encoding:** creates a binary column for each unique category, with a value of 1 indicating the presence of the category and 0 indicating the absence.\n",
        "\n",
        "**Label encoding:** assigns a unique integer value to each category.\n",
        "Ordinal encoding: assigns an ordered integer value to each category based on the natural ordering of the categories.\n",
        "\n",
        "**Count encoding:** replaces a categorical value with the number of times it appears in the dataset."
      ],
      "metadata": {
        "id": "A1aQ2RrxKffX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used onehot encoding & numerical encoding.\n",
        "\n",
        "Numerical encoding and one-hot encoding are two common techniques used for handling categorical data in machine learning. The choice between them depends on the nature of the data and the specific requirements of the machine learning algorithm being used. Here's why each technique is used:\n",
        "\n",
        "- **Numerical Encoding (Label Encoding):**\n",
        "\n",
        "***Ordinal Categorical Data:***\n",
        "Label encoding is used when the categorical data has an inherent ordinal relationship, meaning the categories have a meaningful order. For example, if you have a categorical variable like \"Low,\" \"Medium,\" and \"High\" where \"Low\" < \"Medium\" < \"High,\" you can assign numerical labels like 1, 2, and 3 to represent this order.\n",
        "\n",
        "***Reduction in Dimensionality:***\n",
        "Label encoding can reduce the dimensionality of the dataset when there are many categories. Instead of creating multiple binary columns (as in one-hot encoding), you can represent the categories with a single numerical column.\n",
        "\n",
        "- **One-Hot Encoding:**\n",
        "\n",
        "***Nominal Categorical Data:***\n",
        "One-hot encoding is typically used for nominal categorical data, where there is no intrinsic order among categories. It creates binary columns (0 or 1) for each category, indicating the presence or absence of that category for each data point.\n",
        "\n",
        "***No Assumption of Order:***\n",
        "One-hot encoding doesn't assume any ordinal relationship between categories. It treats each category as a separate and independent feature.\n",
        "\n",
        "***Algorithm Compatibility:***\n",
        "Some machine learning algorithms, especially those based on distance or magnitude (e.g., k-means clustering), work better with one-hot encoded categorical features. These algorithms might misinterpret label-encoded features as having an ordinal relationship.\n",
        "\n",
        "***Interpretability:***\n",
        "One-hot encoding makes the data more interpretable because each category is explicitly represented by its own column."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "wq9uMHJIrcEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As on our dataset there is no column having text/categorical format so there is no need to do contractions."
      ],
      "metadata": {
        "id": "OLNtpZ7tunAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As on our dataset there is no column having text/categorical format so there is no need to do lower casing."
      ],
      "metadata": {
        "id": "WN0b6EA20TBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As on our dataset there is no column having text/categorical format so there is no need to do remove punctuations."
      ],
      "metadata": {
        "id": "vmYqgywS0XDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As on our dataset there is no column having text/categorical format so there is no need to remove urls,remove words."
      ],
      "metadata": {
        "id": "P4R8Oddm0hY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As on our dataset there is no column having text/categorical format so there is no need to remove stopwords."
      ],
      "metadata": {
        "id": "JF4o-6r40ofI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As on our dataset there is no column having text/categorical format so there is no need to rephrase text."
      ],
      "metadata": {
        "id": "nFUf73gF0t_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As on our dataset there is no column having text/categorical format so there is no need to do tokenization."
      ],
      "metadata": {
        "id": "8lGRzEdu0yhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As on our dataset there is no column having text/categorical format so there is no need to do normalization."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As on our dataset there is no column having text/categorical format so there is no need to do speech tagging."
      ],
      "metadata": {
        "id": "VpmSyo4009vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As on our dataset there is no column having text/categorical format so there is no need to do vectorization."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "plt.figure(figsize=(20,15))\n",
        "sns.heatmap(df.corr(),annot=True)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VIF (Variance Inflation Factor) analysis"
      ],
      "metadata": {
        "id": "p1ocYrzpI2rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('Dew_point_temperature',inplace=True,axis=1)"
      ],
      "metadata": {
        "id": "P1OP-m0WTuDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Calculate_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "MrOCNGgvMviW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Calculate_vif(df[[i for i in df.describe().columns if i not in [\"Day\", \"Month\", \"Year\", \"Rented Bike Count\"]]])"
      ],
      "metadata": {
        "id": "UBu0dmxQSfvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Calculate_vif(df[[i for i in df.describe().columns if i not in [\"Day\", \"Month\", \"Year\", \"Rented Bike Count\",\"Functioning_Day\",\"Rainfall\",\"Snowfall\"]]])"
      ],
      "metadata": {
        "id": "SD2iJtbeOurW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Since the VIF factor of \"Day\", \"Month\", \"Year\",\"Functioning_Day\" is too large hence we will remove them from our data to build"
      ],
      "metadata": {
        "id": "P7otH5jzUyf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# droping dew_point_temperature column due to multi-collinearity\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have already dropped dew_point_temperature above ."
      ],
      "metadata": {
        "id": "whIP2NjhGDAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- used heatmap correlation method. in the chart we can see which all are positively corelated to eachother and the values which are mostly corelated that is of no use only increasing the data count.\n",
        "hence we can drop that column.\n",
        "\n",
        "- VIF (Variance Inflation Factor) analysis is a statistical method used to identify multicollinearity in a set of predictor variables. Multicollinearity is a situation where two or more predictor variables in a regression model are highly correlated with each other, meaning that they provide redundant information about the response variable.VIF analysis is an important step in the development of a regression model, as it helps to ensure that the results are reliable and interpretable and that the predictor variables are not providing redundant information.                                in vif method we can see our vif score is not that mush high.so no need to drop any further column."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temperature and Dew point Temperature are highly correlated. As per our regression assumption, there should not be colinearity between independent variables.\n",
        "\n",
        "We can see from the heatmap that \"Temperature\" and \"Dew Point Temperature\" are highly corelated. We can drop one of them.As the corelation between temperature and our dependent variable \"Bike Rented Count\" is high. So we will Keep the Temperature column and drop the \"Dew Point Temperature\" column."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the best transformation for our target variable\n",
        "fig, axs = plt.subplots(1,3, figsize=(16,4))\n",
        "\n",
        "dist1 = sns.distplot(np.log1p(df['Rented Bike Count']),kde=True, ax=axs[0])\n",
        "dist2 = sns.distplot(np.sqrt(df['Rented Bike Count']),kde=True, ax=axs[1])\n",
        "dist3 = sns.distplot(np.cbrt(df['Rented Bike Count']),kde=True, ax=axs[2])\n",
        "# mean line\n",
        "dist2.axvline(np.sqrt(df['Rented Bike Count']).mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "# median line\n",
        "dist2.axvline(np.sqrt(df['Rented Bike Count']).median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "# mean line\n",
        "dist1.axvline(np.log1p(df['Rented Bike Count']).mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "# median line\n",
        "dist1.axvline(np.log1p(df['Rented Bike Count']).median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "# mean line\n",
        "dist3.axvline(np.cbrt(df['Rented Bike Count']).mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "# median line\n",
        "dist3.axvline(np.cbrt(df['Rented Bike Count']).median(), color='black', linestyle='dashed', linewidth=2)"
      ],
      "metadata": {
        "id": "fbKatWkCRlKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Applying a logarithmic transformation to the dependent variable did not help much as it resulted in a negatively skewed distribution.\n",
        "- Square root and cube root transformations were attempted, but they did not result in a normally distributed variable.\n",
        "- Therefore, we will use a square root transformation for the regression as it transformed the variable into a well-distributed form."
      ],
      "metadata": {
        "id": "Yh-jK8faR-vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "dist_new = sns.distplot(np.sqrt(df['Rented Bike Count']))\n",
        "dist_new.set(xlabel='Rented Bike Count',ylabel='Density',title = 'Distribution Plot of Target Variable in sqrt tranformation')\n",
        "# mean line\n",
        "dist_new.axvline(np.sqrt(df['Rented Bike Count']).mean(),color='red')\n",
        "# median line\n",
        "dist_new.axvline(np.sqrt(df['Rented Bike Count']).median(),color='green')\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- By applying the square root transformation to the skewed Rented Bike Count, we were able to obtain an almost normal distribution, which is in line with the general rule that skewed variables should be normalized in linear regression.\n",
        "- We find that there are no outliers in the Rented Bike Count column after applying square root transformation."
      ],
      "metadata": {
        "id": "pEeAf3eIWUbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our data in green plot is normalized to some extent: so we will go with square root on our dependent variable"
      ],
      "metadata": {
        "id": "5Sj1qio4Ruxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# train test split\n",
        "X = df.drop(\"Rented Bike Count\", axis=1)\n",
        "y = df['Rented Bike Count']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2023)\n",
        "\n",
        "print(\"Shape of X_train : \", X_train.shape)\n",
        "print(\"Shape of y_train : \", y_train.shape)\n",
        "print(\"Shape of X_test : \", X_test.shape)\n",
        "print(\"Shape of y_test : \", y_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "UYRFsFfefFog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data splitting ratio used is 80% for training (x_train and y_train) and 20% for testing (x_test and y_test). This is a common splitting ratio in machine learning, where you typically reserve a larger portion of your dataset for training to ensure that your model learns from a sufficient amount of data.\n",
        "\n",
        "The choice of an 80/20 split (or a similar split, such as 70/30 or 90/10) is a trade-off between having enough data to train a robust model and having enough data to evaluate its performance effectively. Here's why this split is commonly used:\n",
        "\n",
        "**Training Data:** The majority of the data (80% in this case) is used for training the machine learning model. A larger training set allows the model to learn the underlying patterns in the data more effectively, which can result in better generalization to unseen data.\n",
        "\n",
        "**Testing Data:** The remaining portion (20% in this case) is reserved for testing or evaluation. This dataset serves as an independent set of examples that the model has never seen during training. It is used to assess how well the model generalizes to new, unseen data.\n",
        "\n",
        "**Trade-off:** While having more training data is beneficial, it's also essential to have a reasonable amount of testing data to evaluate the model's performance accurately. An 80/20 split strikes a balance between these requirements.\n",
        "\n",
        "The random_state parameter is set to ensure reproducibility. By specifying a fixed value for random_state (e.g., random_state=2023), you ensure that the data split will be the same every time you run the code. This is useful for debugging and sharing results, as it makes the code's behavior predictable."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# standard scalar\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "\n",
        "X_train_standard = sc.transform(X_train)\n",
        "X_test_standard = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# minmax scalar\n",
        "scalar_minmax = MinMaxScaler()\n",
        "\n",
        "X_train_minmax = scalar_minmax.fit_transform(X_train)\n",
        "X_test_minmax = scalar_minmax.transform(X_test)"
      ],
      "metadata": {
        "id": "kTnwF4d2I5Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# robust scalar\n",
        "robust_scaler = RobustScaler()\n",
        "\n",
        "X_train_robust = robust_scaler.fit_transform(X_train)\n",
        "X_test_robust = robust_scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "nU_x4cx0KPmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code example I provided, I used three different scaling techniques: StandardScaler, MinMaxScaler, and RobustScaler, and then evaluated their impact on model performance using a logistic regression model.\n",
        "\n",
        "StandardScaler:\n",
        "\n",
        "Method: Scales the data to have a mean of 0 and a standard deviation of 1.\n",
        "When to Use: Use when the data follows a roughly Gaussian distribution and does not have outliers. It's a good default choice for many machine learning algorithms, including linear models, support vector machines, and neural networks.\n",
        "\n",
        "MinMaxScaler:\n",
        "\n",
        "Method: Transforms the data to a specific range, usually [0, 1].\n",
        "When to Use: Use when your data doesn't follow a Gaussian distribution and when your model or algorithm assumes that input features are on the same scale, e.g., neural networks or algorithms sensitive to feature scaling.\n",
        "\n",
        "RobustScaler:\n",
        "\n",
        "Method: Similar to StandardScaler, but uses the median and interquartile range instead of the mean and standard deviation to scale the data. It is less sensitive to the presence of outliers.\n",
        "When to Use: Use when your data contains outliers, and you want to scale the features while being robust to those outliers."
      ],
      "metadata": {
        "id": "aiO88kM8ikhV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reducing dimensionality can lead to improved model performance by reducing the risk of overfitting and making the model more generalizable."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have used. Principal Component Analysis (PCA).\n",
        "y_train variable contains continuous (numeric) values instead of discrete class labels. so we can not use Linear Discriminant Analysis(LDA)."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "# Initialize PCA with the desired number of components (e.g., n_components=2)\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit PCA on the training data and transform both training and test data\n",
        "x_train_pca = pca.fit_transform(X_train_standard)\n",
        "x_test_pca = pca.transform(X_test_standard)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yes target data was imbalanced as we have created a histogram or bar plot of the target variable. Visual inspection can often reveal imbalances. If one class (e.g., high bike demand) significantly outweighs the others (e.g., low bike demand), it may indicate an imbalance."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We handled target class imbalance using square root normalization."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression**"
      ],
      "metadata": {
        "id": "aiEN2k5gIOxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# linear Regression\n",
        "lr = LinearRegression()\n",
        "# Fit the Algorithm\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "gko8TBNHaVJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.score(X_train,y_train)"
      ],
      "metadata": {
        "id": "G_X6P3IxVKar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#after standardization\n",
        "lreg = LinearRegression()\n",
        "# Fit the Algorithm\n",
        "lreg.fit(X_train_standard, y_train)\n",
        "y_pred_standard = lreg.predict(X_test_standard)"
      ],
      "metadata": {
        "id": "HJM7myXxhV3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_standard"
      ],
      "metadata": {
        "id": "0b7O8Vm-hmQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lreg.score(X_train_standard,y_train)"
      ],
      "metadata": {
        "id": "mA3FqCBNhlYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MSE = mean_squared_error(y_test, y_pred)\n",
        "RMSE = np.sqrt(MSE)\n",
        "MAE = mean_absolute_error(y_test, y_pred)\n",
        "R2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"MSE : {MSE}\")\n",
        "print(f\"RMSE : {RMSE}\")\n",
        "print(f\"MAE : {MAE}\")\n",
        "print(f\"R2 : {R2}\")"
      ],
      "metadata": {
        "id": "-F8bZLcRIqXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.title(\"Linear Regression Truth vs Prediction \")\n",
        "plt.xlabel(\"Ground Truth\")\n",
        "plt.ylabel(\"Prediction\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# linear Regression / minmax scalar\n",
        "lreg2 = LinearRegression()\n",
        "# Fit the Algorithm\n",
        "lreg2.fit(X_train_minmax, y_train)\n",
        "y_pred_minmax = lreg2.predict(X_test_minmax)\n"
      ],
      "metadata": {
        "id": "YNhQrLcuEOw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_minmax"
      ],
      "metadata": {
        "id": "NmTLmDM6EerM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lreg2.score(X_train_minmax,y_train)"
      ],
      "metadata": {
        "id": "NQYqpMQPEdp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE = mean_squared_error(y_test, y_pred_minmax)\n",
        "RMSE = np.sqrt(MSE)\n",
        "MAE = mean_absolute_error(y_test, y_pred_minmax)\n",
        "R2 = r2_score(y_test, y_pred_minmax)\n",
        "\n",
        "print(f\"MSE : {MSE}\")\n",
        "print(f\"RMSE : {RMSE}\")\n",
        "print(f\"MAE : {MAE}\")\n",
        "print(f\"R2 : {R2}\")"
      ],
      "metadata": {
        "id": "u84XmkHuEo3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# linear Regression / robust scalar\n",
        "lreg3 = LinearRegression()\n",
        "# Fit the Algorithm\n",
        "lreg3.fit(X_train_robust, y_train)\n",
        "y_pred_robust = lreg3.predict(X_test_robust)"
      ],
      "metadata": {
        "id": "Y0F7NcDTEpyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_robust"
      ],
      "metadata": {
        "id": "cM_JpSllEvTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lreg3.score(X_train_robust,y_train)"
      ],
      "metadata": {
        "id": "NamLS4BsExum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE = mean_squared_error(y_test, y_pred_robust)\n",
        "RMSE = np.sqrt(MSE)\n",
        "MAE = mean_absolute_error(y_test, y_pred_robust)\n",
        "R2 = r2_score(y_test, y_pred_robust)\n",
        "\n",
        "print(f\"MSE : {MSE}\")\n",
        "print(f\"RMSE : {RMSE}\")\n",
        "print(f\"MAE : {MAE}\")\n",
        "print(f\"R2 : {R2}\")"
      ],
      "metadata": {
        "id": "BBIbgu8oE0gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Hyperparameter Tunning using GridSearchCV\n",
        "model = LinearRegression()\n",
        "\n",
        "parameters = {'fit_intercept': [True, False]}\n",
        "linear_grid = GridSearchCV(model, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "\n",
        "# Fit the Algorithm\n",
        "linear_grid.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = linear_grid.best_params_\n",
        "\n",
        "# Get the best model with the tuned hyperparameters\n",
        "best_model = linear_grid.best_estimator_\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_grid = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model"
      ],
      "metadata": {
        "id": "y4AbDmTzwhgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_grid"
      ],
      "metadata": {
        "id": "w9pr8fetwjsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is a commonly used technique for hyperparameter optimization because it exhaustively searches through all possible combinations of hyperparameters in the specified parameter grid.\n",
        "\n",
        "The reason for using GridSearchCV is that it helps you find the best combination of hyperparameters that results in the best model performance. It automates the process of hyperparameter tuning and saves you from manually trying different hyperparameter values.\n",
        "\n",
        "GridSearchCV also performs cross-validation during the hyperparameter search, which helps in estimating how well the model will generalize to unseen data. This helps prevent overfitting.\n",
        "\n",
        "Other hyperparameter optimization techniques, such as RandomizedSearchCV, Bayesian optimization, or manual tuning, can also be used based on the specific problem and dataset. However, GridSearchCV is a good starting point and often provides satisfactory results."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MSE = mean_squared_error(y_test, y_pred_grid)\n",
        "RMSE = np.sqrt(MSE)\n",
        "MAE = mean_absolute_error(y_test, y_pred_grid)\n",
        "r2_linear = r2_score(y_test, y_pred_grid)\n",
        "\n",
        "print(f\"MSE : {MSE}\")\n",
        "print(f\"RMSE : {RMSE}\")\n",
        "print(f\"MAE : {MAE}\")\n",
        "print(f\"R2 : {r2_linear}\")"
      ],
      "metadata": {
        "id": "qA4JzKMtzIfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no such improvement in linear regression.we'll check with another ml models."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ridge Regression**"
      ],
      "metadata": {
        "id": "eSWu2j1pIA66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge regression is a method of estimating the coefficients of regression models in scenarios where the independent variables are highly correlated. It uses the linear regression model with the L2 regularization method.\n",
        "\n"
      ],
      "metadata": {
        "id": "flqbuNFF-4A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ridge Regression\n",
        "ridge = Ridge()\n",
        "# Fit the Algorithm\n",
        "ridge.fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "y_pred_ridge = ridge.predict(X_test)"
      ],
      "metadata": {
        "id": "DQUmWp6YIgEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_ridge"
      ],
      "metadata": {
        "id": "EF-JkvCJKsuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge.score(X_train,y_train)"
      ],
      "metadata": {
        "id": "ftb2gPP3Be5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.scatter(y_test, y_pred_ridge)\n",
        "plt.title(\"Ridge Regression Truth vs Prediction \")\n",
        "plt.xlabel(\"Ground Truth\")\n",
        "plt.ylabel(\"Prediction\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE = mean_squared_error(y_test, y_pred_ridge)\n",
        "RMSE = np.sqrt(MSE)\n",
        "MAE = mean_absolute_error(y_test, y_pred_ridge)\n",
        "R2 = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "print(f\"MSE : {MSE}\")\n",
        "print(f\"RMSE : {RMSE}\")\n",
        "print(f\"MAE : {MAE}\")\n",
        "print(f\"R2 : {R2}\")"
      ],
      "metadata": {
        "id": "jUBWDIsDLEhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge regression using gridsearchCV"
      ],
      "metadata": {
        "id": "z7Km4o5CeSJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Training the ridge regression model using GridSearchCV\n",
        "param_grid = {'alpha': [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100]}\n",
        "ridge_w = GridSearchCV(Ridge(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "ridge_w.fit(X_train,y_train)\n",
        "best_parameter = ridge_w.best_params_"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameter"
      ],
      "metadata": {
        "id": "bPDdBh9UNiz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Algorithm\n",
        "ridge_model=Ridge(alpha=2).fit(X_train,y_train)\n",
        "# Predict on the model\n",
        "y_test_pred_ridge = ridge_model.predict(X_test)"
      ],
      "metadata": {
        "id": "iEt5fi5oNXp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred_ridge"
      ],
      "metadata": {
        "id": "8NS1l2FEN57D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the evaluation metrics for Ridge Regression\n",
        "mse = mean_squared_error(y_test, y_test_pred_ridge)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_test_pred_ridge)\n",
        "r2_ridge = r2_score(y_test, y_test_pred_ridge)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"R-squared (R2) Score: {r2_ridge}\")"
      ],
      "metadata": {
        "id": "9L1AmgGQVeDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge regression using RandomizedSearchCV"
      ],
      "metadata": {
        "id": "1RkYKSxOezQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the ridge regression model using RandomizedSearchCV\n",
        "# Create a Ridge Regression model\n",
        "param_grid = {'alpha': [0.1, 0.2, 0.5, 10, 20, 5, 100, 30, 500, 1000]}\n",
        "random_search = RandomizedSearchCV(Ridge(random_state=50), param_distributions=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "random_search.fit(X_train,y_train)\n",
        "best_Randomizedparameter = random_search.best_params_"
      ],
      "metadata": {
        "id": "x-7hjlpBPvqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_Randomizedparameter"
      ],
      "metadata": {
        "id": "EngZnXAGS4eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Algorithm\n",
        "ridge_Randomized_model=Ridge(alpha=0.5).fit(X_train,y_train)\n",
        "# Predict on the model\n",
        "y_test_pred_ridge_Randomized = ridge_Randomized_model.predict(X_test)"
      ],
      "metadata": {
        "id": "vF9mK-fRS9hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred_ridge_Randomized"
      ],
      "metadata": {
        "id": "mguK_iZlT1jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(y_test, y_test_pred_ridge_Randomized)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_test_pred_ridge_Randomized)\n",
        "r2 = r2_score(y_test, y_test_pred_ridge_Randomized)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"R-squared (R2) Score: {r2}\")"
      ],
      "metadata": {
        "id": "anFWtz8LVf_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have used RandomizedSearchCV & GridsearchCV.\n",
        "Using Ridge regression our prediction and score got improved , there is also difference after using hyperparameters."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  After using ridge regression prediction and score got improved to 87%.\n",
        "\n",
        "-  When we used hyperparameter (gridsearchCV) our % increased to 92%.\n",
        "\n",
        "-  using hyperparameter (randomsearchCV) prediction not got increased rather it decreased to 85%."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are getting the same evaluation metrics (such as MSE, RMSE, MAE, and R-squared) for both Ridge Regression and Linear Regression models, it suggests that the two models are performing very similarly on your dataset. This can happen for several reasons.\n",
        "\n",
        " If Ridge Regression doesn't provide a clear advantage in terms of performance or model interpretability, it's perfectly valid to choose the simpler Linear Regression model.\n",
        "\n",
        " But after using hyperparameter in ridge regression and linear regression prediction score increased to good level.\n",
        "\n",
        " Improving the prediction scores of your Ridge Regression and Linear Regression models through hyperparameter tuning can have several positive business impacts, depending on the specific context and application of your models. Here are some potential business impacts:\n",
        " -  Increased Accuracy: Higher prediction scores generally indicate that your models are better at making accurate predictions. In business, accurate predictions can lead to better decision-making.\n",
        " -  Cost Reduction: Improved accuracy can lead to cost savings.\n",
        " -  Enhanced Customer Experience: When predictions are more accurate, businesses can provide better services to their customers.\n",
        "\n",
        " Overall, the business impact of improved prediction scores can be substantial. It can result in better financial performance, improved customer satisfaction, reduced risks, and a more competitive position in the market. However, it's important to continually monitor and validate the model's performance to ensure that it delivers the expected benefits over time."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lasso Regression**"
      ],
      "metadata": {
        "id": "PH8qM9UJHweP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso regression analysis is a shrinkage and variable selection method for linear regression models. The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error for a quantitative response variable. It uses the Linear regression model with L1 regularization."
      ],
      "metadata": {
        "id": "UKjihDrhxC4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "lasso = Lasso()\n",
        "# Fit the Algorithm\n",
        "lasso.fit(X_train,y_train)\n",
        "# Predict on the model\n",
        "y_pred_lasso = lasso.predict(X_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_lasso"
      ],
      "metadata": {
        "id": "RFfnGevZyG7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso.score(X_train,y_train)"
      ],
      "metadata": {
        "id": "i4Lq8yA8yIeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.scatter(y_test, y_pred_lasso)\n",
        "plt.title(\"Lasso Regression Truth vs Prediction \")\n",
        "plt.xlabel(\"Ground Truth\")\n",
        "plt.ylabel(\"Prediction\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(y_test, y_pred_lasso)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred_lasso)\n",
        "r2 = r2_score(y_test, y_pred_lasso)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"R-squared (R2) Score: {r2}\")"
      ],
      "metadata": {
        "id": "WDzWk14cysj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# ML Model - 3 hyperparameter optimization techniques RandomSearch CV\n",
        "\n",
        "params_grid = {'alpha': [0.8,1,10,20,40,80,100,300,700,1000]}\n",
        "lasso_grid_search = GridSearchCV(Lasso(random_state=40), params_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "lasso_grid_search.fit(X_train,y_train)\n",
        "best_lasso_parameter = lasso_grid_search.best_params_\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_lasso_parameter"
      ],
      "metadata": {
        "id": "PyK61eqr0Yk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Algorithm\n",
        "lasso_model=Lasso(alpha=0.8).fit(X_train,y_train)\n",
        "# Predict on the model\n",
        "y_test_pred_lasso = lasso_model.predict(X_test)"
      ],
      "metadata": {
        "id": "39x2O5nQ0Zla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred_lasso"
      ],
      "metadata": {
        "id": "sPHUw-wT07Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(y_test, y_test_pred_lasso)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_test_pred_lasso)\n",
        "r2 = r2_score(y_test, y_test_pred_lasso)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"R-squared (R2) Score: {r2}\")"
      ],
      "metadata": {
        "id": "uBPBis1p1Y8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 hyperparameter optimization techniques RandomSearch CV\n",
        "params_grid = {'alpha': [0.5,10,100,200,400,800,100,300,700,1000]}\n",
        "lasso_Randomized_search = RandomizedSearchCV(Lasso(random_state=42), params_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "lasso_Randomized_search.fit(X_train,y_train)\n",
        "randomized_lasso_parameter = lasso_grid_search.best_params_"
      ],
      "metadata": {
        "id": "SiaUWflb291X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomized_lasso_parameter"
      ],
      "metadata": {
        "id": "c-HWqWF-4fhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Algorithm\n",
        "lasso_model=Lasso(alpha=0.8).fit(X_train,y_train)\n",
        "# Predict on the model\n",
        "y_randomized_pred_lasso = lasso_model.predict(X_test)"
      ],
      "metadata": {
        "id": "0bbjVA7D4iEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_randomized_pred_lasso"
      ],
      "metadata": {
        "id": "xo-bE6X855Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(y_test, y_randomized_pred_lasso)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_randomized_pred_lasso)\n",
        "r2_lasso = r2_score(y_test, y_randomized_pred_lasso)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"R-squared (R2) Score: {r2_lasso}\")"
      ],
      "metadata": {
        "id": "r9Fl87jO59oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both RandomsearchCV and GridsearchCV we have used and both are giving same results."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both results are same.\n",
        "\n",
        "evaluation metrics are also same after hyperparameter."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classfication Metrics Report**"
      ],
      "metadata": {
        "id": "B2Q1Osk_Hmlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification metrics are used to evaluate the performance of a classification model by comparing the predicted labels to the actual labels. Accuracy can be useful in evaluating sentiment analysis models, particularly if the classes are balanced.\n",
        "\n",
        "Accuracy: The proportion of correctly predicted labels out of the total number of samples. It is computed as (TP+TN)/(TP+TN+FP+FN)\n",
        "\n",
        "Where\n",
        "\n",
        "TP:-True Positive\n",
        "\n",
        "TN:-True Negative\n",
        "\n",
        "FP:-False Positive\n",
        "\n",
        "FN:-False Negative"
      ],
      "metadata": {
        "id": "JUrwXtFJHiRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acurracy = {'Model':  ['Linear Regression','Ridge Regression', 'Lasso Regression'],\n",
        "        'Count Vector':  [r2_linear,r2_ridge,r2_lasso] , 'Prediction score': [y_pred_grid,y_test_pred_ridge,y_randomized_pred_lasso]}\n",
        "\n",
        "cv_score_table= pd.DataFrame (acurracy, columns = ['Model','Count Vector','Prediction score'])\n",
        "cv_score_table"
      ],
      "metadata": {
        "id": "cCYxIBinHWrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge regression we'll use for a better positive impact on business."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we'll choose ridge regression with hyperparameter (gridsearchCV) as final prediction model.\n",
        "as it gives good metrics score."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model explainability refers to the concept of being able to understand the machine learning model. For example – If a healthcare model is predicting whether a patient is suffering from a particular disease or not. The medical practitioners need to know what parameters the model is taking into account or if the model contains any bias. So, it is necessary that once the model is deployed in the real world. Then, the model developers can explain the model.\n",
        "\n",
        "Popular techniques for model explainability:\n",
        "\n",
        "- LIME\n",
        "- SHAP\n",
        "- ELI-5\n",
        "\n",
        "In this project I'll be using SHAP for model explainability. Among the various methods in SHAP I'll be using the SHAP summary plot, which plots features/columns in order of their impact on the predictions and also plots the SHAP values."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a function to plot the shap summary plot\n",
        "def shap_summary(model):\n",
        "   explainer_shap = shap.Explainer(model=model, masker=X_train)\n",
        "   shap_values = explainer_shap.shap_values(X_train)\n",
        "   shap.summary_plot(shap_values,X_train,feature_names=X.columns)"
      ],
      "metadata": {
        "id": "d4SilKmfEdpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting shap summary plot for linear regression\n",
        "shap_summary(lr)"
      ],
      "metadata": {
        "id": "PQP5DYQNEhud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting shap summary plot for Ridge regression\n",
        "shap_summary(ridge)"
      ],
      "metadata": {
        "id": "vX_usSi6Emh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting shap summary plot for Lasso regression\n",
        "shap_summary(lasso)"
      ],
      "metadata": {
        "id": "Eo2rK-xvEpP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By looking at the SHAP summary plot for each model, we can figure out the feature importance and also its impact power by understanding the SHAP values."
      ],
      "metadata": {
        "id": "8pRshIGIGU0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. EDA insights:\n",
        "- Most number of bikes are rented in the Summer season and the lowest in the winter season.\n",
        "- Over 96% of the bikes are rented on days that are considered as No Holiday.\n",
        "- Most number of bikes are rented in the temperature range of 15 degrees to 30 degrees.\n",
        "- Most number of bikes are rented when there is no snowfall or rainfall.\n",
        "- Majority of the bikes are rented for a humidity percentage range of 30 to 70.\n",
        "- The highest number of bike rentals have been done in the 18th hour, i.e 6pm, and lowest in the 4th hour, i.e 4am.\n",
        "- Most of the bike rentals have been made when there is high visibility.\n",
        "\n",
        "2. Results from ML models:\n",
        "- Ridge Regression is the best performing model with an r2 score of 0.53 and prediction 87%.\n",
        "- Lasso Regression(L1 regularization) is the worst performing model with an r2 score & low prediction.\n",
        "- Actual vs Prediction visualisation is done for all the 3 models.\n",
        "- All 3 models have been explained with the help of SHAP library.\n",
        "- Temperature and Hour are the two most important factors according to all the models.\n",
        "\n",
        "3. Challenges faced:\n",
        "- Removing Outliers.\n",
        "- Encoding the categorical columns.\n",
        "- Removing Multicollinearity from the dataset.\n",
        "- Choosing Model explainability technique."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}